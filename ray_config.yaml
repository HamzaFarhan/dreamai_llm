# This file was generated using the `serve build` command on Ray v2.7.0.

proxy_location: EveryNode

http_options:
  host: 0.0.0.0

  port: 8889

grpc_options:
  port: 9000

  grpc_servicer_functions: []

applications:
  - name: app1

    route_prefix: /

    import_path: deploy:deployment_handle

    runtime_env: {}

    deployments:
      - name: AppIngress
        user_config:
          redis_host: 127.0.0.1
          redis_port: 6379
          llm_config:
            llm_fn: vllm
            model_name: meta-llama/Llama-2-7b-chat-hf
            # token: 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
            model_url: http://localhost:8000/v1
        autoscaling_config:
          min_replicas: 2
          initial_replicas: null
          max_replicas: 6
          target_num_ongoing_requests_per_replica: 5.0
          metrics_interval_s: 10.0
          look_back_period_s: 30.0
          smoothing_factor: 1.0
          upscale_smoothing_factor: null
          downscale_smoothing_factor: null
          downscale_delay_s: 600.0
          upscale_delay_s: 30.0
        health_check_period_s: 10.0
        health_check_timeout_s: 30.0
        ray_actor_options:
          num_cpus: 2.0
